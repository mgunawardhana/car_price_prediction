{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gradio as gr\n",
    "import traceback\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv('/content/drive/MyDrive/ML/Car price prediction/train.csv')\n",
    "    test_df = pd.read_csv('/content/drive/MyDrive/ML/Car price prediction/test.csv')\n",
    "    sample_submission_df = pd.read_csv('/content/drive/MyDrive/ML/Car price prediction/sample_submission.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure train.csv, test.csv, and sample_submission.csv are uploaded to your Colab environment.\")\n",
    "    raise\n",
    "\n",
    "# --- EDA PLOTS for Coursework Task (LO2) ---\n",
    "# Detect target column\n",
    "possible_target_cols = [col for col in train_df.columns if col.lower() in ['price', 'Price', 'cost', 'target', 'price_usd', 'sale_price', 'value', 'car_price', 'price_in_usd', 'amount']]\n",
    "if not possible_target_cols:\n",
    "    print(f\"Columns in train_df: {list(train_df.columns)}\")\n",
    "    raise ValueError(\"No target column found in train_df.\")\n",
    "target_col = possible_target_cols[0]\n",
    "print(f\"\\n[EDA] Using target column for plots: {target_col}\")\n",
    "\n",
    "# 1. Price Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df[target_col], bins=50, kde=True, color='orange')\n",
    "plt.title('Price Distribution')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Scatter Plot: Car Age vs Price\n",
    "if 'year' in [col.lower() for col in train_df.columns]:\n",
    "    year_col = [col for col in train_df.columns if 'year' in col.lower()][0]\n",
    "    train_df['car_age'] = 2024 - train_df[year_col]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='car_age', y=target_col, data=train_df, alpha=0.5, color='teal')\n",
    "    plt.title('Car Age vs Price')\n",
    "    plt.xlabel('Car Age')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- 1. Enhanced Data Preprocessing ---\n",
    "def advanced_preprocessing(train_df, test_df):\n",
    "    possible_target_cols = [col for col in train_df.columns if col.lower() in ['price', 'Price', 'cost', 'target', 'price_usd', 'sale_price', 'value', 'car_price', 'price_in_usd', 'amount']]\n",
    "    if not possible_target_cols:\n",
    "        print(f\"Columns in train_df: {list(train_df.columns)}\")\n",
    "        raise ValueError(\"No target column (e.g., 'price') found.\")\n",
    "    target_col = possible_target_cols[0]\n",
    "    print(f\"Using target column: {target_col}\")\n",
    "\n",
    "    X = train_df.drop(target_col, axis=1)\n",
    "    y = train_df[target_col]\n",
    "    test_ids = test_df['Id'] if 'Id' in test_df.columns else test_df.index\n",
    "    X_test = test_df.drop('Id', axis=1) if 'Id' in test_df.columns else test_df\n",
    "    combined_df = pd.concat([X, X_test], ignore_index=True)\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in combined_df.columns:\n",
    "        if combined_df[col].dtype == 'object':\n",
    "            combined_df[col] = combined_df[col].fillna('Unknown')\n",
    "        else:\n",
    "            combined_df[col] = combined_df[col].fillna(combined_df[col].median())\n",
    "\n",
    "    numerical_features = combined_df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = combined_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Create derived features\n",
    "    if 'year' in [col.lower() for col in numerical_features]:\n",
    "        year_col = [col for col in numerical_features if 'year' in col.lower()][0]\n",
    "        combined_df['car_age'] = 2024 - combined_df[year_col]\n",
    "\n",
    "    if any('mileage' in col.lower() or 'km' in col.lower() for col in numerical_features):\n",
    "        mileage_col = [col for col in numerical_features if 'mileage' in col.lower() or 'km' in col.lower()][0]\n",
    "        combined_df['mileage_per_year'] = combined_df[mileage_col] / (combined_df.get('car_age', 1) + 1)\n",
    "\n",
    "    if any('motor' in col.lower() or 'engine' in col.lower() for col in numerical_features):\n",
    "        motor_col = [col for col in numerical_features if 'motor' in col.lower() or 'engine' in col.lower()][0]\n",
    "        combined_df['motor_efficiency'] = combined_df[motor_col] * 1000\n",
    "\n",
    "    # Update feature lists after creating derived features\n",
    "    numerical_features = combined_df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = combined_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Store encoders for later use\n",
    "    encoders = {}\n",
    "    encoded_features = []\n",
    "\n",
    "    for cat_col in categorical_features:\n",
    "        if combined_df[cat_col].nunique() > 10:\n",
    "            le = LabelEncoder()\n",
    "            combined_df[f'{cat_col}_encoded'] = le.fit_transform(combined_df[cat_col])\n",
    "            encoders[cat_col] = {'type': 'label', 'encoder': le}\n",
    "            encoded_features.append(f'{cat_col}_encoded')\n",
    "        else:\n",
    "            encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "            encoded_cats = encoder.fit_transform(combined_df[[cat_col]])\n",
    "            encoded_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out([cat_col]))\n",
    "            combined_df = pd.concat([combined_df, encoded_df], axis=1)\n",
    "            encoders[cat_col] = {'type': 'onehot', 'encoder': encoder}\n",
    "            encoded_features.extend(encoded_df.columns)\n",
    "\n",
    "    final_features = list(numerical_features) + encoded_features\n",
    "    final_combined_df = combined_df[final_features].fillna(0)\n",
    "    X_processed = final_combined_df.iloc[:len(train_df)]\n",
    "    X_test_processed = final_combined_df.iloc[len(train_df):]\n",
    "\n",
    "    return X_processed, X_test_processed, y, test_ids, final_features, encoders\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed, X_test_processed, y, test_ids, final_features, encoders = advanced_preprocessing(train_df, test_df)\n",
    "\n",
    "print(f\"Advanced preprocessing completed.\")\n",
    "print(f\"Training data shape: {X_processed.shape}\")\n",
    "print(f\"Test data shape: {X_test_processed.shape}\")\n",
    "print(f\"Number of features: {len(final_features)}\")\n",
    "\n",
    "# --- 2. Outlier Detection and Handling ---\n",
    "def remove_outliers(X, y, method='iqr'):\n",
    "    if method == 'iqr':\n",
    "        Q1 = y.quantile(0.25)\n",
    "        Q3 = y.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        mask = (y >= lower_bound) & (y <= upper_bound)\n",
    "        return X[mask], y[mask]\n",
    "    return X, y\n",
    "\n",
    "X_processed_clean, y_clean = remove_outliers(X_processed, y)\n",
    "print(f\"After outlier removal: {X_processed_clean.shape[0]} samples (removed {len(X_processed) - len(X_processed_clean)} outliers)\")\n",
    "\n",
    "# --- 3. Feature Scaling ---\n",
    "scaler = StandardScaler()\n",
    "X_processed_scaled = scaler.fit_transform(X_processed_clean)\n",
    "X_test_processed_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "# --- 4. Enhanced Model Training ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed_scaled, y_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    'n_estimators': 1500,\n",
    "    'learning_rate': 0.03,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'colsample_bylevel': 0.85,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(\"\\nTraining enhanced XGBoost model...\")\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "y_pred_val = xgb_model.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"\\nEnhanced XGBoost Validation Results:\")\n",
    "print(f\"Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
    "print(f\"R-squared (RÂ²) Score: {r2:.4f}\")\n",
    "\n",
    "# --- 5. Ensemble Model ---\n",
    "print(\"\\nTraining ensemble model...\")\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "ridge_model = Ridge(alpha=10.0, random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_pred = xgb_model.predict(X_val)\n",
    "rf_pred = rf_model.predict(X_val)\n",
    "ridge_pred = ridge_model.predict(X_val)\n",
    "\n",
    "ensemble_pred = 0.6 * xgb_pred + 0.3 * rf_pred + 0.1 * ridge_pred\n",
    "ensemble_mae = mean_absolute_error(y_val, ensemble_pred)\n",
    "ensemble_r2 = r2_score(y_val, ensemble_pred)\n",
    "\n",
    "print(f\"\\nEnsemble Model Validation Results:\")\n",
    "print(f\"Mean Absolute Error (MAE): ${ensemble_mae:,.2f}\")\n",
    "print(f\"R-squared (RÂ²) Score: {ensemble_r2:.4f}\")\n",
    "\n",
    "if ensemble_mae < mae:\n",
    "    print(\"\\nEnsemble model performs better!\")\n",
    "    best_model_type = \"ensemble\"\n",
    "    best_mae = ensemble_mae\n",
    "    best_r2 = ensemble_r2\n",
    "else:\n",
    "    print(\"\\nXGBoost model performs better!\")\n",
    "    best_model_type = \"xgboost\"\n",
    "    best_mae = mae\n",
    "    best_r2 = r2\n",
    "\n",
    "# --- 6. Final Model Training on Full Dataset ---\n",
    "print(f\"\\nRetraining best model ({best_model_type}) on full dataset...\")\n",
    "\n",
    "xgb_model_final = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=xgb_model.best_iteration if hasattr(xgb_model, 'best_iteration') else 1000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=8,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    colsample_bylevel=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model_final.fit(X_processed_scaled, y_clean)\n",
    "\n",
    "rf_model_final = None\n",
    "ridge_model_final = None\n",
    "\n",
    "if best_model_type == \"ensemble\":\n",
    "    rf_model_final = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "    rf_model_final.fit(X_processed_scaled, y_clean)\n",
    "\n",
    "    ridge_model_final = Ridge(alpha=10.0, random_state=42)\n",
    "    ridge_model_final.fit(X_processed_scaled, y_clean)\n",
    "\n",
    "# --- 7. Final Predictions ---\n",
    "print(\"Making final predictions on test set...\")\n",
    "\n",
    "if best_model_type == \"ensemble\":\n",
    "    xgb_final_pred = xgb_model_final.predict(X_test_processed_scaled)\n",
    "    rf_final_pred = rf_model_final.predict(X_test_processed_scaled)\n",
    "    ridge_final_pred = ridge_model_final.predict(X_test_processed_scaled)\n",
    "    final_predictions = 0.6 * xgb_final_pred + 0.3 * rf_final_pred + 0.1 * ridge_final_pred\n",
    "else:\n",
    "    final_predictions = xgb_model_final.predict(X_test_processed_scaled)\n",
    "\n",
    "submission_df = pd.DataFrame({'Id': test_ids, 'Price': final_predictions})\n",
    "submission_df['Price'] = submission_df['Price'].clip(lower=0)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Best Model: {best_model_type.title()}\")\n",
    "print(f\"Best MAE: ${best_mae:,.2f}\")\n",
    "print(f\"Best RÂ²: {best_r2:.4f}\")\n",
    "print(f\"Submission file 'submission.csv' created successfully!\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Store column information for UI\n",
    "original_columns = [col for col in train_df.columns if col != target_col]\n",
    "\n",
    "def preprocess_single_input(input_dict, encoders, final_features):\n",
    "    \"\"\"\n",
    "    Preprocess a single input for prediction using the same logic as training\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Input received: {input_dict}\")\n",
    "\n",
    "        # Create DataFrame from input\n",
    "        input_df = pd.DataFrame([input_dict])\n",
    "        print(f\"Input DataFrame shape: {input_df.shape}\")\n",
    "        print(f\"Input DataFrame:\\n{input_df}\")\n",
    "\n",
    "        # Handle missing values and convert data types\n",
    "        for col in input_df.columns:\n",
    "            if input_df[col].dtype == 'object':\n",
    "                input_df[col] = input_df[col].fillna('Unknown')\n",
    "            else:\n",
    "                try:\n",
    "                    input_df[col] = pd.to_numeric(input_df[col], errors='coerce')\n",
    "                    input_df[col] = input_df[col].fillna(0)\n",
    "                except:\n",
    "                    input_df[col] = input_df[col].fillna('Unknown')\n",
    "\n",
    "        print(f\"After type conversion:\\n{input_df}\")\n",
    "\n",
    "        # Create derived features (same as training)\n",
    "        numerical_features = input_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        # Car age feature\n",
    "        if 'year' in [col.lower() for col in numerical_features]:\n",
    "            year_col = [col for col in numerical_features if 'year' in col.lower()][0]\n",
    "            input_df['car_age'] = 2024 - input_df[year_col]\n",
    "            print(f\"Created car_age: {input_df['car_age'].iloc[0]}\")\n",
    "\n",
    "        # Mileage per year feature\n",
    "        if any('mileage' in col.lower() or 'km' in col.lower() for col in numerical_features):\n",
    "            mileage_col = [col for col in numerical_features if 'mileage' in col.lower() or 'km' in col.lower()][0]\n",
    "            car_age = input_df.get('car_age', pd.Series([1])).iloc[0]\n",
    "            input_df['mileage_per_year'] = input_df[mileage_col] / (car_age + 1)\n",
    "            print(f\"Created mileage_per_year: {input_df['mileage_per_year'].iloc[0]}\")\n",
    "\n",
    "        # Motor efficiency feature\n",
    "        if any('motor' in col.lower() or 'engine' in col.lower() for col in numerical_features):\n",
    "            motor_col = [col for col in numerical_features if 'motor' in col.lower() or 'engine' in col.lower()][0]\n",
    "            input_df['motor_efficiency'] = input_df[motor_col] * 1000\n",
    "            print(f\"Created motor_efficiency: {input_df['motor_efficiency'].iloc[0]}\")\n",
    "\n",
    "        # Apply encodings\n",
    "        for col in input_df.select_dtypes(include=['object']).columns:\n",
    "            if col in encoders:\n",
    "                if encoders[col]['type'] == 'label':\n",
    "                    try:\n",
    "                        # Handle unknown categories for label encoder\n",
    "                        value = input_df[col].iloc[0]\n",
    "                        if value in encoders[col]['encoder'].classes_:\n",
    "                            input_df[f'{col}_encoded'] = encoders[col]['encoder'].transform([value])\n",
    "                        else:\n",
    "                            # Use the most frequent class or 0 for unknown values\n",
    "                            input_df[f'{col}_encoded'] = 0\n",
    "                        print(f\"Label encoded {col}: {input_df[f'{col}_encoded'].iloc[0]}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Label encoding error for {col}: {e}\")\n",
    "                        input_df[f'{col}_encoded'] = 0\n",
    "                else:  # OneHot encoding\n",
    "                    try:\n",
    "                        encoded_cats = encoders[col]['encoder'].transform(input_df[[col]])\n",
    "                        encoded_df = pd.DataFrame(encoded_cats, columns=encoders[col]['encoder'].get_feature_names_out([col]))\n",
    "                        input_df = pd.concat([input_df, encoded_df], axis=1)\n",
    "                        print(f\"One-hot encoded {col}: {encoded_df.shape[1]} features\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"One-hot encoding error for {col}: {e}\")\n",
    "                        # Create dummy columns with zeros\n",
    "                        feature_names = encoders[col]['encoder'].get_feature_names_out([col])\n",
    "                        for feature_name in feature_names:\n",
    "                            input_df[feature_name] = 0\n",
    "\n",
    "        print(f\"After encoding, DataFrame shape: {input_df.shape}\")\n",
    "        print(f\"After encoding columns: {list(input_df.columns)}\")\n",
    "\n",
    "        # Select only the features used in training\n",
    "        missing_features = []\n",
    "        final_input = pd.DataFrame()\n",
    "\n",
    "        for feature in final_features:\n",
    "            if feature in input_df.columns:\n",
    "                final_input[feature] = input_df[feature]\n",
    "            else:\n",
    "                final_input[feature] = 0  # Fill missing features with 0\n",
    "                missing_features.append(feature)\n",
    "\n",
    "        if missing_features:\n",
    "            print(f\"Missing features filled with 0: {missing_features[:10]}...\")  # Show first 10\n",
    "\n",
    "        print(f\"Final input shape: {final_input.shape}\")\n",
    "        print(f\"Expected features: {len(final_features)}\")\n",
    "\n",
    "        return final_input\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Preprocessing error: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise e\n",
    "\n",
    "def create_prediction_ui():\n",
    "    \"\"\"\n",
    "    Creates a Gradio UI for car price prediction with better error handling\n",
    "    \"\"\"\n",
    "\n",
    "    def predict_car_price(*args):\n",
    "        try:\n",
    "            print(f\"Received {len(args)} arguments\")\n",
    "            print(f\"Original columns: {original_columns}\")\n",
    "\n",
    "            # Create input dictionary from arguments\n",
    "            input_dict = {}\n",
    "            for i, col in enumerate(original_columns):\n",
    "                if i < len(args):\n",
    "                    input_dict[col] = args[i]\n",
    "                    print(f\"{col}: {args[i]} (type: {type(args[i])})\")\n",
    "\n",
    "            # Preprocess the input\n",
    "            processed_input = preprocess_single_input(input_dict, encoders, final_features)\n",
    "\n",
    "            # Scale the features\n",
    "            scaled_input = scaler.transform(processed_input)\n",
    "            print(f\"Scaled input shape: {scaled_input.shape}\")\n",
    "\n",
    "            # Make prediction\n",
    "            if best_model_type == \"ensemble\":\n",
    "                xgb_pred = xgb_model_final.predict(scaled_input)[0]\n",
    "                rf_pred = rf_model_final.predict(scaled_input)[0]\n",
    "                ridge_pred = ridge_model_final.predict(scaled_input)[0]\n",
    "                prediction = 0.6 * xgb_pred + 0.3 * rf_pred + 0.1 * ridge_pred\n",
    "                print(f\"Ensemble predictions - XGB: {xgb_pred:.2f}, RF: {rf_pred:.2f}, Ridge: {ridge_pred:.2f}\")\n",
    "            else:\n",
    "                prediction = xgb_model_final.predict(scaled_input)[0]\n",
    "                print(f\"XGBoost prediction: {prediction:.2f}\")\n",
    "\n",
    "            # Ensure non-negative price\n",
    "            prediction = max(0, prediction)\n",
    "\n",
    "            result = f\" Predicted Car Price: ${prediction:,.2f}\"\n",
    "            print(f\"Final prediction: {result}\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\" Prediction Error: {str(e)}\"\n",
    "            print(f\"Error in predict_car_price: {error_msg}\")\n",
    "            print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "            return error_msg\n",
    "\n",
    "    # Create input components based on original columns\n",
    "    input_components = []\n",
    "\n",
    "    for col in original_columns:\n",
    "        try:\n",
    "            # Get unique values from test_df or train_df\n",
    "            if col in test_df.columns:\n",
    "                unique_values = pd.concat([train_df[col], test_df[col]]).dropna().unique()\n",
    "            else:\n",
    "                unique_values = train_df[col].dropna().unique()\n",
    "\n",
    "            # Convert to strings and sort\n",
    "            if len(unique_values) > 0:\n",
    "                if pd.api.types.is_numeric_dtype(unique_values):\n",
    "                    # For numerical columns, create a number input\n",
    "                    min_val = float(unique_values.min())\n",
    "                    max_val = float(unique_values.max())\n",
    "                    default_val = float(np.median(unique_values))\n",
    "                    input_components.append(\n",
    "                        gr.Number(\n",
    "                            label=col,\n",
    "                            value=default_val,\n",
    "                            minimum=min_val,\n",
    "                            maximum=max_val,\n",
    "                            step=1 if col.lower() in ['year'] else None\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    # For categorical columns, create dropdown\n",
    "                    unique_values_str = [str(val) for val in unique_values if str(val) != 'nan']\n",
    "                    unique_values_str = sorted(list(set(unique_values_str)))\n",
    "                    if len(unique_values_str) == 0:\n",
    "                        unique_values_str = ['Unknown']\n",
    "                    input_components.append(\n",
    "                        gr.Dropdown(\n",
    "                            label=col,\n",
    "                            choices=unique_values_str,\n",
    "                            value=unique_values_str[0]\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                # Fallback for empty columns\n",
    "                input_components.append(gr.Textbox(label=col, value=\"Unknown\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating component for {col}: {e}\")\n",
    "            input_components.append(gr.Textbox(label=col, value=\"Unknown\"))\n",
    "\n",
    "    # Create the Gradio interface\n",
    "    with gr.Blocks(title=\"Car Price Prediction\", theme=gr.themes.Soft()) as interface:\n",
    "        gr.Markdown(\"# Car Price Prediction System\")\n",
    "        gr.Markdown(\"Enter car details below to get a price prediction.\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"###  Car Details\")\n",
    "\n",
    "                # Split inputs into multiple columns for better layout\n",
    "                mid_point = len(input_components) // 2\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        for component in input_components[:mid_point]:\n",
    "                            component.render()\n",
    "\n",
    "                    with gr.Column():\n",
    "                        for component in input_components[mid_point:]:\n",
    "                            component.render()\n",
    "\n",
    "                predict_button = gr.Button(\"ð® Predict Price\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### Model Information\")\n",
    "                gr.Markdown(f\"\"\"\n",
    "                **Model Type:** {best_model_type.title()}\n",
    "                **Validation MAE:** ${best_mae:,.2f}\n",
    "                **Validation RÂ²:** {best_r2:.4f}\n",
    "                **Features Used:** {len(final_features)}\n",
    "                \"\"\")\n",
    "\n",
    "                gr.Markdown(\"### Prediction Result\")\n",
    "                output = gr.Textbox(\n",
    "                    label=\"Predicted Price\",\n",
    "                    placeholder=\"Click 'Predict Price' to see the result...\",\n",
    "                    interactive=False,\n",
    "                    lines=2\n",
    "                )\n",
    "\n",
    "        # Connect the prediction function\n",
    "        predict_button.click(\n",
    "            fn=predict_car_price,\n",
    "            inputs=input_components,\n",
    "            outputs=output\n",
    "        )\n",
    "\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### How it works:\n",
    "        1. **Select/Enter** car details in the form above\n",
    "        2. **Click** 'Predict Price' to get the estimated value\n",
    "        3. **Model** uses advanced ML algorithms trained on car data\n",
    "        \"\"\")\n",
    "\n",
    "    return interface\n",
    "\n",
    "# Launch the UI\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ð LAUNCHING CAR PRICE PREDICTION UI\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    ui = create_prediction_ui()\n",
    "    print(\"UI created successfully!\")\n",
    "    print(\"Launching Gradio interface...\")\n",
    "    ui.launch(share=True, debug=True)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to launch UI: {str(e)}\")\n",
    "    print(f\"Full error: {traceback.format_exc()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
